{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=10>\n",
    "    Massive Data Mining <br>\n",
    "<font color=0F5298 size=5>\n",
    "    Electrical Engineering Department <br>\n",
    "    Fall 2024 <br>\n",
    "    Parham Gilani - 400101859 <br>\n",
    "    Homework 2 - Spark Exercise <br>\n",
    "    \n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from hashlib import sha256\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from itertools import combinations\n",
    "import hashlib\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore') \n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"MDA2024-HW2\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"id\":\"0704.0001\",\"submitter\":\"Pavel Nadolsky\",\"authors\":\"C. Bal\\\\\\\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\"title\":\"Calculation of prompt diphoton production cross sections at Tevatron and\\\\n  LHC energies\",\"comments\":\"37 pages, 15 figures; published version\",\"journal-ref\":\"Phys.Rev.D76:013009,2007\",\"doi\":\"10.1103/PhysRevD.76.013009\",\"report-no\":\"ANL-HEP-PR-07-12\",\"categories\":\"hep-ph\",\"license\":null,\"abstract\":\"  A fully differential calculation in perturbative quantum chromodynamics is\\\\npresented for the production of massive photon pairs at hadron colliders. All\\\\nnext-to-leading order perturbative contributions from quark-antiquark,\\\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\\\nall-orders resummation of initial-state gluon radiation valid at\\\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\\\nspecified in which the calculation is most reliable. Good agreement is\\\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\\\nmore detailed tests with CDF and DO data. Predictions are shown for\\\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\\\nthat enhanced sensitivity to the signal can be obtained with judicious\\\\nselection of events.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 19:18:42 GMT\"},{\"version\":\"v2\",\"created\":\"Tue, 24 Jul 2007 20:10:27 GMT\"}],\"update_date\":\"2008-11-26\",\"authors_parsed\":[[\"Bal\\\\u00e1zs\",\"C.\",\"\"],[\"Berger\",\"E. L.\",\"\"],[\"Nadolsky\",\"P. M.\",\"\"],[\"Yuan\",\"C. -P.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0002\",\"submitter\":\"Louis Theran\",\"authors\":\"Ileana Streinu and Louis Theran\",\"title\":\"Sparsity-certifying Graph Decompositions\",\"comments\":\"To appear in Graphs and Combinatorics\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO cs.CG\",\"license\":\"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\",\"abstract\":\"  We describe a new algorithm, the $(k,\\\\\\\\ell)$-pebble game with colors, and use\\\\nit obtain a characterization of the family of $(k,\\\\\\\\ell)$-sparse graphs and\\\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\\\nreceived increased attention in recent years. In particular, our colored\\\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\\\nalso present a new decomposition that certifies sparsity based on the\\\\n$(k,\\\\\\\\ell)$-pebble game with colors. Our work also exposes connections between\\\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\\\nWestermann and Hendrickson.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 02:26:18 GMT\"},{\"version\":\"v2\",\"created\":\"Sat, 13 Dec 2008 17:26:00 GMT\"}],\"update_date\":\"2008-12-13\",\"authors_parsed\":[[\"Streinu\",\"Ileana\",\"\"],[\"Theran\",\"Louis\",\"\"]]}',\n",
       " '{\"id\":\"0704.0003\",\"submitter\":\"Hongjun Pan\",\"authors\":\"Hongjun Pan\",\"title\":\"The evolution of the Earth-Moon system based on the dark matter field\\\\n  fluid model\",\"comments\":\"23 pages, 3 figures\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"physics.gen-ph\",\"license\":null,\"abstract\":\"  The evolution of Earth-Moon system is described by the dark matter field\\\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\\\nwith this model very well and the general pattern of the evolution of the\\\\nMoon-Earth system described by this model agrees with geological and fossil\\\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\\\nbillion years ago, which is far beyond the Roche\\'s limit. The result suggests\\\\nthat the tidal friction may not be the primary cause for the evolution of the\\\\nEarth-Moon system. The average dark matter field fluid constant derived from\\\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\\\nthat the Mars\\'s rotation is also slowing with the angular acceleration rate\\\\nabout -4.38 x 10^(-22) rad s^(-2).\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sun, 1 Apr 2007 20:46:54 GMT\"},{\"version\":\"v2\",\"created\":\"Sat, 8 Dec 2007 23:47:24 GMT\"},{\"version\":\"v3\",\"created\":\"Sun, 13 Jan 2008 00:36:28 GMT\"}],\"update_date\":\"2008-01-13\",\"authors_parsed\":[[\"Pan\",\"Hongjun\",\"\"]]}',\n",
       " '{\"id\":\"0704.0004\",\"submitter\":\"David Callan\",\"authors\":\"David Callan\",\"title\":\"A determinant of Stirling cycle numbers counts unlabeled acyclic\\\\n  single-source automata\",\"comments\":\"11 pages\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO\",\"license\":null,\"abstract\":\"  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\\\nsingle-source automata. The proof involves a bijection from these automata to\\\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\\\ndeterminant.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 03:16:14 GMT\"}],\"update_date\":\"2007-05-23\",\"authors_parsed\":[[\"Callan\",\"David\",\"\"]]}',\n",
       " '{\"id\":\"0704.0005\",\"submitter\":\"Alberto Torchinsky\",\"authors\":\"Wael Abu-Shammala and Alberto Torchinsky\",\"title\":\"From dyadic $\\\\\\\\Lambda_{\\\\\\\\alpha}$ to $\\\\\\\\Lambda_{\\\\\\\\alpha}$\",\"comments\":null,\"journal-ref\":\"Illinois J. Math. 52 (2008) no.2, 681-689\",\"doi\":null,\"report-no\":null,\"categories\":\"math.CA math.FA\",\"license\":null,\"abstract\":\"  In this paper we show how to compute the $\\\\\\\\Lambda_{\\\\\\\\alpha}$ norm, $\\\\\\\\alpha\\\\\\\\ge\\\\n0$, using the dyadic grid. This result is a consequence of the description of\\\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 18:09:58 GMT\"}],\"update_date\":\"2013-10-15\",\"authors_parsed\":[[\"Abu-Shammala\",\"Wael\",\"\"],[\"Torchinsky\",\"Alberto\",\"\"]]}',\n",
       " '{\"id\":\"0704.0006\",\"submitter\":\"Yue Hin Pong\",\"authors\":\"Y. H. Pong and C. K. Law\",\"title\":\"Bosonic characters of atomic Cooper pairs across resonance\",\"comments\":\"6 pages, 4 figures, accepted by PRA\",\"journal-ref\":null,\"doi\":\"10.1103/PhysRevA.75.043613\",\"report-no\":null,\"categories\":\"cond-mat.mes-hall\",\"license\":null,\"abstract\":\"  We study the two-particle wave function of paired atoms in a Fermi gas with\\\\ntunable interaction strengths controlled by Feshbach resonance. The Cooper pair\\\\nwave function is examined for its bosonic characters, which is quantified by\\\\nthe correction of Bose enhancement factor associated with the creation and\\\\nannihilation composite particle operators. An example is given for a\\\\nthree-dimensional uniform gas. Two definitions of Cooper pair wave function are\\\\nexamined. One of which is chosen to reflect the off-diagonal long range order\\\\n(ODLRO). Another one corresponds to a pair projection of a BCS state. On the\\\\nside with negative scattering length, we found that paired atoms described by\\\\nODLRO are more bosonic than the pair projected definition. It is also found\\\\nthat at $(k_F a)^{-1} \\\\\\\\ge 1$, both definitions give similar results, where more\\\\nthan 90% of the atoms occupy the corresponding molecular condensates.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 04:24:59 GMT\"}],\"update_date\":\"2015-05-13\",\"authors_parsed\":[[\"Pong\",\"Y. H.\",\"\"],[\"Law\",\"C. K.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0007\",\"submitter\":\"Alejandro Corichi\",\"authors\":\"Alejandro Corichi, Tatjana Vukasinac and Jose A. Zapata\",\"title\":\"Polymer Quantum Mechanics and its Continuum Limit\",\"comments\":\"16 pages, no figures. Typos corrected to match published version\",\"journal-ref\":\"Phys.Rev.D76:044016,2007\",\"doi\":\"10.1103/PhysRevD.76.044016\",\"report-no\":\"IGPG-07/03-2\",\"categories\":\"gr-qc\",\"license\":null,\"abstract\":\"  A rather non-standard quantum representation of the canonical commutation\\\\nrelations of quantum mechanics systems, known as the polymer representation has\\\\ngained some attention in recent years, due to its possible relation with Planck\\\\nscale physics. In particular, this approach has been followed in a symmetric\\\\nsector of loop quantum gravity known as loop quantum cosmology. Here we explore\\\\ndifferent aspects of the relation between the ordinary Schroedinger theory and\\\\nthe polymer description. The paper has two parts. In the first one, we derive\\\\nthe polymer quantum mechanics starting from the ordinary Schroedinger theory\\\\nand show that the polymer description arises as an appropriate limit. In the\\\\nsecond part we consider the continuum limit of this theory, namely, the reverse\\\\nprocess in which one starts from the discrete theory and tries to recover back\\\\nthe ordinary Schroedinger quantum mechanics. We consider several examples of\\\\ninterest, including the harmonic oscillator, the free particle and a simple\\\\ncosmological model.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 04:27:22 GMT\"},{\"version\":\"v2\",\"created\":\"Wed, 22 Aug 2007 22:42:11 GMT\"}],\"update_date\":\"2008-11-26\",\"authors_parsed\":[[\"Corichi\",\"Alejandro\",\"\"],[\"Vukasinac\",\"Tatjana\",\"\"],[\"Zapata\",\"Jose A.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0008\",\"submitter\":\"Damian Swift\",\"authors\":\"Damian C. Swift\",\"title\":\"Numerical solution of shock and ramp compression for general material\\\\n  properties\",\"comments\":\"Minor corrections\",\"journal-ref\":\"Journal of Applied Physics, vol 104, 073536 (2008)\",\"doi\":\"10.1063/1.2975338\",\"report-no\":\"LA-UR-07-2051, LLNL-JRNL-410358\",\"categories\":\"cond-mat.mtrl-sci\",\"license\":\"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\",\"abstract\":\"  A general formulation was developed to represent material models for\\\\napplications in dynamic loading. Numerical methods were devised to calculate\\\\nresponse to shock and ramp compression, and ramp decompression, generalizing\\\\nprevious solutions for scalar equations of state. The numerical methods were\\\\nfound to be flexible and robust, and matched analytic results to a high\\\\naccuracy. The basic ramp and shock solution methods were coupled to solve for\\\\ncomposite deformation paths, such as shock-induced impacts, and shock\\\\ninteractions with a planar interface between different materials. These\\\\ncalculations capture much of the physics of typical material dynamics\\\\nexperiments, without requiring spatially-resolving simulations. Example\\\\ncalculations were made of loading histories in metals, illustrating the effects\\\\nof plastic work on the temperatures induced in quasi-isentropic and\\\\nshock-release experiments, and the effect of a phase transition.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 04:47:20 GMT\"},{\"version\":\"v2\",\"created\":\"Thu, 10 Apr 2008 08:42:28 GMT\"},{\"version\":\"v3\",\"created\":\"Tue, 1 Jul 2008 18:54:28 GMT\"}],\"update_date\":\"2009-02-05\",\"authors_parsed\":[[\"Swift\",\"Damian C.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0009\",\"submitter\":\"Paul Harvey\",\"authors\":\"Paul Harvey, Bruno Merin, Tracy L. Huard, Luisa M. Rebull, Nicholas\\\\n  Chapman, Neal J. Evans II, Philip C. Myers\",\"title\":\"The Spitzer c2d Survey of Large, Nearby, Insterstellar Clouds. IX. The\\\\n  Serpens YSO Population As Observed With IRAC and MIPS\",\"comments\":null,\"journal-ref\":\"Astrophys.J.663:1149-1173,2007\",\"doi\":\"10.1086/518646\",\"report-no\":null,\"categories\":\"astro-ph\",\"license\":null,\"abstract\":\"  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy\\\\nobservations of the Serpens star-forming region. In particular we present a set\\\\nof criteria for isolating bona fide young stellar objects, YSO\\'s, from the\\\\nextensive background contamination by extra-galactic objects. We then discuss\\\\nthe properties of the resulting high confidence set of YSO\\'s. We find 235 such\\\\nobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An\\\\nadditional set of 51 lower confidence YSO\\'s outside this area is identified\\\\nfrom the MIPS data combined with 2MASS photometry. We describe two sets of\\\\nresults, color-color diagrams to compare our observed source properties with\\\\nthose of theoretical models for star/disk/envelope systems and our own modeling\\\\nof the subset of our objects that appear to be star+disks. These objects\\\\nexhibit a very wide range of disk properties, from many that can be fit with\\\\nactively accreting disks to some with both passive disks and even possibly\\\\ndebris disks. We find that the luminosity function of YSO\\'s in Serpens extends\\\\ndown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.\\\\nThe lower limit may be set by our inability to distinguish YSO\\'s from\\\\nextra-galactic sources more than by the lack of YSO\\'s at very low luminosities.\\\\nA spatial clustering analysis shows that the nominally less-evolved YSO\\'s are\\\\nmore highly clustered than the later stages and that the background\\\\nextra-galactic population can be fit by the same two-point correlation function\\\\nas seen in other extra-galactic studies. We also present a table of matches\\\\nbetween several previous infrared and X-ray studies of the Serpens YSO\\\\npopulation and our Spitzer data set.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 19:41:34 GMT\"}],\"update_date\":\"2010-03-18\",\"authors_parsed\":[[\"Harvey\",\"Paul\",\"\"],[\"Merin\",\"Bruno\",\"\"],[\"Huard\",\"Tracy L.\",\"\"],[\"Rebull\",\"Luisa M.\",\"\"],[\"Chapman\",\"Nicholas\",\"\"],[\"Evans\",\"Neal J.\",\"II\"],[\"Myers\",\"Philip C.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0010\",\"submitter\":\"Sergei Ovchinnikov\",\"authors\":\"Sergei Ovchinnikov\",\"title\":\"Partial cubes: structures, characterizations, and constructions\",\"comments\":\"36 pages, 17 figures\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO\",\"license\":null,\"abstract\":\"  Partial cubes are isometric subgraphs of hypercubes. Structures on a graph\\\\ndefined by means of semicubes, and Djokovi\\\\\\\\\\'{c}\\'s and Winkler\\'s relations play\\\\nan important role in the theory of partial cubes. These structures are employed\\\\nin the paper to characterize bipartite graphs and partial cubes of arbitrary\\\\ndimension. New characterizations are established and new proofs of some known\\\\nresults are given.\\\\n  The operations of Cartesian product and pasting, and expansion and\\\\ncontraction processes are utilized in the paper to construct new partial cubes\\\\nfrom old ones. In particular, the isometric and lattice dimensions of finite\\\\npartial cubes obtained by means of these operations are calculated.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 05:10:16 GMT\"}],\"update_date\":\"2007-05-23\",\"authors_parsed\":[[\"Ovchinnikov\",\"Sergei\",\"\"]]}']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_rdd = sc.textFile(\"MDA2024-Arxiv-Dataset.json\")\n",
    "arxiv_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON string\n",
    "def parse_json(line):\n",
    "    return json.loads(line)\n",
    "\n",
    "parsed_rdd = arxiv_rdd.map(parse_json).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove or impute null values\n",
    "def remove_nulls(record):\n",
    "    # Check for null or empty fields in the record\n",
    "    if all(record.values()):  # Ensure no field is None or empty\n",
    "        return record\n",
    "    else:\n",
    "        return None  # Ignore records with null or empty fields\n",
    "\n",
    "cleaned_rdd = parsed_rdd.filter(lambda x: remove_nulls(x) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of stopwords for removal\n",
    "stopwords = set(StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "\n",
    "def remove_stopwords(record):\n",
    "    for key, value in record.items():\n",
    "        if isinstance(value, str):  # Process only text fields\n",
    "            words = value.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "            record[key] = \" \".join(filtered_words)\n",
    "    return record\n",
    "\n",
    "stopwords_removed_rdd = cleaned_rdd.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '07040008',\n",
       "  'submitter': 'Damian Swift',\n",
       "  'authors': 'Damian C Swift',\n",
       "  'title': 'Numerical solution shock ramp compression general material properties',\n",
       "  'comments': 'Minor corrections',\n",
       "  'journal-ref': 'Journal Applied Physics vol 104 073536 2008',\n",
       "  'doi': '10106312975338',\n",
       "  'report-no': 'LAUR072051 LLNLJRNL410358',\n",
       "  'categories': 'condmatmtrlsci',\n",
       "  'license': 'httparxivorglicensesnonexclusivedistrib10',\n",
       "  'abstract': 'general formulation developed represent material models applications dynamic loading Numerical methods devised calculate response shock ramp compression ramp decompression generalizing previous solutions scalar equations state numerical methods found flexible robust matched analytic results high accuracy basic ramp shock solution methods coupled solve composite deformation paths shockinduced impacts shock interactions planar interface different materials calculations capture much physics typical material dynamics experiments without requiring spatiallyresolving simulations Example calculations made loading histories metals illustrating effects plastic work temperatures induced quasiisentropic shockrelease experiments effect phase transition',\n",
       "  'versions': [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 04:47:20 GMT'},\n",
       "   {'version': 'v2', 'created': 'Thu, 10 Apr 2008 08:42:28 GMT'},\n",
       "   {'version': 'v3', 'created': 'Tue, 1 Jul 2008 18:54:28 GMT'}],\n",
       "  'update_date': '20090205',\n",
       "  'authors_parsed': [['Swift', 'Damian C.', '']]}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_useless_characters(record):\n",
    "    for key, value in record.items():\n",
    "        if isinstance(value, str):  # Process only text fields\n",
    "            # Remove special characters, retaining only alphanumeric and spaces\n",
    "            record[key] = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", value)\n",
    "    return record\n",
    "\n",
    "final_cleaned_rdd = stopwords_removed_rdd.map(remove_useless_characters)\n",
    "\n",
    "# Save or view the final cleaned RDD\n",
    "final_cleaned_rdd.take(1)  # View the first cleaned records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in sampled RDD (15.0%): 6091\n"
     ]
    }
   ],
   "source": [
    "# Get a 1% sample from final_cleaned_rdd\n",
    "sample_fraction = 0.15\n",
    "sampled_rdd = final_cleaned_rdd.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "# Check how many records are in the sampled RDD\n",
    "sampled_count = sampled_rdd.count()\n",
    "print(f\"Number of records in sampled RDD ({sample_fraction*100}%): {sampled_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: ('dark', 'interactions', 'matter'), Count: 1682\n",
      "Trigram: ('TeV', 'mass', 'search'), Count: 1084\n",
      "Trigram: ('dark', 'matter', 'parameter'), Count: 1194\n",
      "Trigram: ('GeV', 'boson', 'mass'), Count: 1220\n",
      "Trigram: ('dark', 'matter', 'symmetry'), Count: 1381\n",
      "Trigram: ('dark', 'matter', 'searches'), Count: 1510\n",
      "Trigram: ('Higgs', 'boson', 'vector'), Count: 1132\n",
      "Trigram: ('Higgs', 'TeV', 'boson'), Count: 1781\n",
      "Trigram: ('LHC', 'dark', 'matter'), Count: 1109\n",
      "Trigram: ('TeV', 'mass', 'model'), Count: 1049\n",
      "Trigram: ('Higgs', 'boson', 'bosons'), Count: 1251\n",
      "Trigram: ('dark', 'matter', 'two'), Count: 1616\n",
      "Trigram: ('dark', 'matter', 'show'), Count: 1218\n",
      "Trigram: ('field', 'theories', 'theory'), Count: 1171\n",
      "Trigram: ('consider', 'dark', 'matter'), Count: 1209\n",
      "Trigram: ('dark', 'matter', 'range'), Count: 1008\n",
      "Trigram: ('dark', 'matter', 'new'), Count: 1044\n",
      "Trigram: ('Higgs', 'mass', 'model'), Count: 1033\n",
      "Trigram: ('Higgs', 'boson', 'standard'), Count: 1481\n",
      "Trigram: ('field', 'gauge', 'theory'), Count: 1377\n",
      "Trigram: ('dark', 'matter', 'models'), Count: 2523\n",
      "Trigram: ('GeV', 'mass', 'masses'), Count: 1011\n",
      "Trigram: ('data', 'model', 'models'), Count: 1652\n",
      "Trigram: ('Higgs', 'boson', 'integrated'), Count: 1006\n",
      "Trigram: ('dark', 'matter', 'observed'), Count: 1432\n",
      "Trigram: ('production', 'quark', 'top'), Count: 1282\n",
      "Trigram: ('dark', 'matter', 'produced'), Count: 1015\n",
      "Trigram: ('TeV', 'limits', 'search'), Count: 1068\n",
      "Trigram: ('gauge', 'theories', 'theory'), Count: 1044\n",
      "Trigram: ('Higgs', 'LHC', 'boson'), Count: 1353\n",
      "Trigram: ('dark', 'masses', 'matter'), Count: 2018\n",
      "Trigram: ('cross', 'production', 'section'), Count: 1016\n",
      "Trigram: ('dark', 'light', 'matter'), Count: 1478\n",
      "Trigram: ('dark', 'gauge', 'matter'), Count: 1677\n",
      "Trigram: ('branching', 'pm', 'to'), Count: 1647\n",
      "Trigram: ('boson', 'dark', 'matter'), Count: 1038\n",
      "Trigram: ('GeV', 'Higgs', 'boson'), Count: 1741\n",
      "Trigram: ('dark', 'find', 'matter'), Count: 1852\n",
      "Trigram: ('couplings', 'dark', 'matter'), Count: 1096\n",
      "Trigram: ('dark', 'data', 'matter'), Count: 1529\n",
      "Trigram: ('dark', 'energy', 'matter'), Count: 1622\n",
      "Trigram: ('dark', 'matter', 'model'), Count: 4140\n",
      "Trigram: ('dark', 'direct', 'matter'), Count: 2244\n",
      "Trigram: ('Higgs', 'Standard', 'boson'), Count: 1243\n",
      "Trigram: ('Higgs', 'mass', 'production'), Count: 1066\n",
      "Trigram: ('dark', 'matter', 'scenario'), Count: 1340\n",
      "Trigram: ('also', 'dark', 'matter'), Count: 2462\n",
      "Trigram: ('mass', 'matter', 'model'), Count: 1123\n",
      "Trigram: ('Higgs', 'boson', 'model'), Count: 2024\n",
      "Trigram: ('dark', 'matter', 'particles'), Count: 1912\n",
      "Trigram: ('dark', 'matter', 'scalar'), Count: 1966\n",
      "Trigram: ('dark', 'matter', 'study'), Count: 1239\n",
      "Trigram: ('Higgs', 'boson', 'decay'), Count: 1497\n",
      "Trigram: ('Higgs', 'boson', 'search'), Count: 1551\n",
      "Trigram: ('Higgs', 'boson', 'production'), Count: 3372\n",
      "Trigram: ('Higgs', 'boson', 'observed'), Count: 1293\n",
      "Trigram: ('boson', 'production', 'search'), Count: 1123\n",
      "Trigram: ('TeV', 'data', 'mass'), Count: 1021\n",
      "Trigram: ('Higgs', 'dark', 'matter'), Count: 1973\n",
      "Trigram: ('TeV', 'boson', 'production'), Count: 1193\n",
      "Trigram: ('Higgs', 'TeV', 'production'), Count: 1046\n",
      "Trigram: ('dark', 'density', 'matter'), Count: 1463\n",
      "Trigram: ('dark', 'matter', 'sector'), Count: 1482\n",
      "Trigram: ('Model', 'dark', 'matter'), Count: 1225\n",
      "Trigram: ('constraints', 'dark', 'matter'), Count: 2357\n",
      "Trigram: ('Higgs', 'boson', 'pair'), Count: 1357\n",
      "Trigram: ('dark', 'matter', 'particle'), Count: 2270\n",
      "Trigram: ('dark', 'matter', 'scale'), Count: 1286\n",
      "Trigram: ('GeV', 'mass', 'model'), Count: 1003\n",
      "Trigram: ('boson', 'mass', 'production'), Count: 1130\n",
      "Trigram: ('dark', 'limits', 'matter'), Count: 1195\n",
      "Trigram: ('GeV', 'TeV', 'mass'), Count: 1062\n",
      "Trigram: ('dark', 'matter', 'neutrino'), Count: 2174\n",
      "Trigram: ('Higgs', 'Model', 'Standard'), Count: 1397\n",
      "Trigram: ('Higgs', 'boson', 'mass'), Count: 2104\n",
      "Trigram: ('dark', 'mass', 'model'), Count: 1117\n",
      "Trigram: ('B', 'branching', 'to'), Count: 1060\n",
      "Trigram: ('GeV', 'mass', 'search'), Count: 1033\n",
      "Trigram: ('dark', 'discuss', 'matter'), Count: 1300\n",
      "Trigram: ('dark', 'matter', 'scattering'), Count: 1102\n",
      "Trigram: ('Standard', 'dark', 'matter'), Count: 1148\n",
      "Trigram: ('Higgs', 'Model', 'boson'), Count: 1422\n",
      "Trigram: ('dark', 'matter', 'may'), Count: 1243\n",
      "Trigram: ('dark', 'decay', 'matter'), Count: 2187\n",
      "Trigram: ('GeV', 'dark', 'matter'), Count: 2216\n",
      "Trigram: ('dark', 'mass', 'matter'), Count: 4923\n",
      "Trigram: ('dark', 'experiments', 'matter'), Count: 1709\n",
      "Trigram: ('dark', 'decays', 'matter'), Count: 1180\n",
      "Trigram: ('Higgs', 'boson', 'data'), Count: 1523\n",
      "Trigram: ('effective', 'field', 'theory'), Count: 1109\n",
      "Trigram: ('mass', 'quark', 'top'), Count: 1257\n",
      "Trigram: ('dark', 'matter', 'production'), Count: 1165\n",
      "Trigram: ('black', 'hole', 'mass'), Count: 1458\n",
      "Trigram: ('Higgs', 'boson', 'decays'), Count: 1267\n",
      "Trigram: ('dark', 'model', 'scalar'), Count: 1065\n",
      "Trigram: ('TeV', 'mass', 'production'), Count: 1034\n",
      "Trigram: ('Higgs', 'boson', 'cross'), Count: 1189\n",
      "Trigram: ('field', 'quantum', 'theory'), Count: 1065\n",
      "Trigram: ('TeV', 'limits', 'mass'), Count: 1149\n",
      "Trigram: ('Higgs', 'analysis', 'boson'), Count: 1019\n",
      "Trigram: ('Higgs', 'boson', 'signal'), Count: 1027\n",
      "Trigram: ('mass', 'masses', 'model'), Count: 1125\n",
      "Trigram: ('GeV', 'Higgs', 'mass'), Count: 1289\n",
      "Trigram: ('GeV', 'mass', 'top'), Count: 1276\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "first_pass_support_threshold = 300  # Support threshold for frequent words in the first pass\n",
    "second_pass_support_threshold = 1000  # Support threshold for frequent trigrams in the second pass\n",
    "\n",
    "# First pass: Count frequent words\n",
    "def first_pass_apriori(rdd, support_threshold):\n",
    "    # Count occurrences of words\n",
    "    word_counts = rdd.flatMap(lambda record: record['abstract'].split()) \\\n",
    "                     .map(lambda word: (word, 1)) \\\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    # Find frequent words\n",
    "    frequent_words = word_counts.filter(lambda x: x[1] >= support_threshold) \\\n",
    "                                .map(lambda x: x[0]) \\\n",
    "                                .collect()\n",
    "    frequent_word_set = set(frequent_words)\n",
    "    return frequent_word_set\n",
    "\n",
    "# Second pass: Generate and count candidate trigrams\n",
    "def second_pass_apriori(rdd, frequent_word_set, support_threshold):\n",
    "    # Generate and count trigrams\n",
    "    candidate_trigrams = rdd.flatMap(\n",
    "        lambda record: [\n",
    "            tuple(sorted(trigram))\n",
    "            for trigram in combinations(record['abstract'].split(), 3)\n",
    "            if len(set(trigram)) == 3 and set(trigram).issubset(frequent_word_set)\n",
    "        ]\n",
    "    ).map(lambda trigram: (trigram, 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    # Filter trigrams based on support threshold\n",
    "    frequent_trigrams = candidate_trigrams.filter(lambda x: x[1] >= support_threshold)\n",
    "    return frequent_trigrams\n",
    "\n",
    "# First pass: Find frequent words\n",
    "frequent_word_set = first_pass_apriori(sampled_rdd, first_pass_support_threshold)\n",
    "\n",
    "# Second pass: Generate and filter frequent trigrams\n",
    "frequent_trigrams_apriori = second_pass_apriori(sampled_rdd, frequent_word_set, second_pass_support_threshold)\n",
    "\n",
    "# Collect and display results\n",
    "results = frequent_trigrams_apriori.collect()\n",
    "for trigram, count in results:\n",
    "    print(f\"Trigram: {trigram}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **First Pass: Identifying Frequent Words**\n",
    "1. **Tokenization and Counting:**  \n",
    "   - Each abstract is split into individual words.  \n",
    "   - The occurrences of each word across all abstracts are counted using PySpark transformations (`flatMap`, `map`, `reduceByKey`).  \n",
    "\n",
    "2. **Filtering Frequent Words:**  \n",
    "   - Words that appear fewer times than the specified `first_pass_support_threshold` (e.g., 300 occurrences) are discarded.  \n",
    "   - A set of frequent words is returned, which is used in the second pass to restrict the scope of analysis.\n",
    "\n",
    "### **Second Pass: Identifying Frequent Trigrams**\n",
    "1. **Trigram Generation:**  \n",
    "   - For each abstract, all possible trigrams (combinations of three unique words) are generated using Python's `itertools.combinations`.  \n",
    "   - Only trigrams where all three words are in the frequent word set (from the first pass) are retained, ensuring efficiency.  \n",
    "\n",
    "2. **Counting Trigram Occurrences:**  \n",
    "   - The occurrences of each trigram across all abstracts are counted using similar transformations.  \n",
    "   \n",
    "3. **Filtering Frequent Trigrams:**  \n",
    "   - Trigrams with occurrences below the `second_pass_support_threshold` (e.g., 1000) are discarded.  \n",
    "   - The remaining frequent trigrams are collected as the final result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram: ('dark', 'interactions', 'matter'), Count: 1682\n",
      "Trigram: ('TeV', 'mass', 'search'), Count: 1084\n",
      "Trigram: ('dark', 'matter', 'parameter'), Count: 1194\n",
      "Trigram: ('GeV', 'boson', 'mass'), Count: 1220\n",
      "Trigram: ('dark', 'matter', 'symmetry'), Count: 1381\n",
      "Trigram: ('dark', 'matter', 'searches'), Count: 1510\n",
      "Trigram: ('Higgs', 'boson', 'vector'), Count: 1132\n",
      "Trigram: ('Higgs', 'TeV', 'boson'), Count: 1781\n",
      "Trigram: ('LHC', 'dark', 'matter'), Count: 1109\n",
      "Trigram: ('TeV', 'mass', 'model'), Count: 1049\n",
      "Trigram: ('Higgs', 'boson', 'bosons'), Count: 1251\n",
      "Trigram: ('dark', 'matter', 'two'), Count: 1616\n",
      "Trigram: ('dark', 'matter', 'show'), Count: 1218\n",
      "Trigram: ('field', 'theories', 'theory'), Count: 1171\n",
      "Trigram: ('consider', 'dark', 'matter'), Count: 1209\n",
      "Trigram: ('dark', 'matter', 'range'), Count: 1008\n",
      "Trigram: ('dark', 'matter', 'new'), Count: 1044\n",
      "Trigram: ('Higgs', 'mass', 'model'), Count: 1033\n",
      "Trigram: ('Higgs', 'boson', 'standard'), Count: 1481\n",
      "Trigram: ('field', 'gauge', 'theory'), Count: 1377\n",
      "Trigram: ('dark', 'matter', 'models'), Count: 2523\n",
      "Trigram: ('GeV', 'mass', 'masses'), Count: 1011\n",
      "Trigram: ('data', 'model', 'models'), Count: 1652\n",
      "Trigram: ('Higgs', 'boson', 'integrated'), Count: 1006\n",
      "Trigram: ('dark', 'matter', 'observed'), Count: 1432\n",
      "Trigram: ('production', 'quark', 'top'), Count: 1282\n",
      "Trigram: ('dark', 'matter', 'produced'), Count: 1015\n",
      "Trigram: ('TeV', 'limits', 'search'), Count: 1068\n",
      "Trigram: ('gauge', 'theories', 'theory'), Count: 1044\n",
      "Trigram: ('Higgs', 'LHC', 'boson'), Count: 1353\n",
      "Trigram: ('dark', 'masses', 'matter'), Count: 2018\n",
      "Trigram: ('cross', 'production', 'section'), Count: 1016\n",
      "Trigram: ('dark', 'light', 'matter'), Count: 1478\n",
      "Trigram: ('dark', 'gauge', 'matter'), Count: 1677\n",
      "Trigram: ('branching', 'pm', 'to'), Count: 1647\n",
      "Trigram: ('boson', 'dark', 'matter'), Count: 1038\n",
      "Trigram: ('GeV', 'Higgs', 'boson'), Count: 1741\n",
      "Trigram: ('dark', 'find', 'matter'), Count: 1852\n",
      "Trigram: ('couplings', 'dark', 'matter'), Count: 1096\n",
      "Trigram: ('dark', 'data', 'matter'), Count: 1529\n",
      "Trigram: ('dark', 'energy', 'matter'), Count: 1622\n",
      "Trigram: ('dark', 'matter', 'model'), Count: 4140\n",
      "Trigram: ('dark', 'direct', 'matter'), Count: 2244\n",
      "Trigram: ('Higgs', 'Standard', 'boson'), Count: 1243\n",
      "Trigram: ('Higgs', 'mass', 'production'), Count: 1066\n",
      "Trigram: ('dark', 'matter', 'scenario'), Count: 1340\n",
      "Trigram: ('also', 'dark', 'matter'), Count: 2462\n",
      "Trigram: ('mass', 'matter', 'model'), Count: 1123\n",
      "Trigram: ('Higgs', 'boson', 'model'), Count: 2024\n",
      "Trigram: ('dark', 'matter', 'particles'), Count: 1912\n",
      "Trigram: ('dark', 'matter', 'scalar'), Count: 1966\n",
      "Trigram: ('dark', 'matter', 'study'), Count: 1239\n",
      "Trigram: ('Higgs', 'boson', 'decay'), Count: 1497\n",
      "Trigram: ('Higgs', 'boson', 'search'), Count: 1551\n",
      "Trigram: ('Higgs', 'boson', 'production'), Count: 3372\n",
      "Trigram: ('Higgs', 'boson', 'observed'), Count: 1293\n",
      "Trigram: ('boson', 'production', 'search'), Count: 1123\n",
      "Trigram: ('TeV', 'data', 'mass'), Count: 1021\n",
      "Trigram: ('Higgs', 'dark', 'matter'), Count: 1973\n",
      "Trigram: ('TeV', 'boson', 'production'), Count: 1193\n",
      "Trigram: ('Higgs', 'TeV', 'production'), Count: 1046\n",
      "Trigram: ('dark', 'density', 'matter'), Count: 1463\n",
      "Trigram: ('dark', 'matter', 'sector'), Count: 1482\n",
      "Trigram: ('Model', 'dark', 'matter'), Count: 1225\n",
      "Trigram: ('constraints', 'dark', 'matter'), Count: 2357\n",
      "Trigram: ('Higgs', 'boson', 'pair'), Count: 1357\n",
      "Trigram: ('dark', 'matter', 'particle'), Count: 2270\n",
      "Trigram: ('dark', 'matter', 'scale'), Count: 1286\n",
      "Trigram: ('GeV', 'mass', 'model'), Count: 1003\n",
      "Trigram: ('boson', 'mass', 'production'), Count: 1130\n",
      "Trigram: ('dark', 'limits', 'matter'), Count: 1195\n",
      "Trigram: ('GeV', 'TeV', 'mass'), Count: 1062\n",
      "Trigram: ('dark', 'matter', 'neutrino'), Count: 2174\n",
      "Trigram: ('Higgs', 'Model', 'Standard'), Count: 1397\n",
      "Trigram: ('Higgs', 'boson', 'mass'), Count: 2104\n",
      "Trigram: ('dark', 'mass', 'model'), Count: 1117\n",
      "Trigram: ('B', 'branching', 'to'), Count: 1060\n",
      "Trigram: ('GeV', 'mass', 'search'), Count: 1033\n",
      "Trigram: ('dark', 'discuss', 'matter'), Count: 1300\n",
      "Trigram: ('dark', 'matter', 'scattering'), Count: 1102\n",
      "Trigram: ('Standard', 'dark', 'matter'), Count: 1148\n",
      "Trigram: ('Higgs', 'Model', 'boson'), Count: 1422\n",
      "Trigram: ('dark', 'matter', 'may'), Count: 1243\n",
      "Trigram: ('dark', 'decay', 'matter'), Count: 2187\n",
      "Trigram: ('GeV', 'dark', 'matter'), Count: 2216\n",
      "Trigram: ('dark', 'mass', 'matter'), Count: 4923\n",
      "Trigram: ('dark', 'experiments', 'matter'), Count: 1709\n",
      "Trigram: ('dark', 'decays', 'matter'), Count: 1180\n",
      "Trigram: ('Higgs', 'boson', 'data'), Count: 1523\n",
      "Trigram: ('effective', 'field', 'theory'), Count: 1109\n",
      "Trigram: ('mass', 'quark', 'top'), Count: 1257\n",
      "Trigram: ('dark', 'matter', 'production'), Count: 1165\n",
      "Trigram: ('black', 'hole', 'mass'), Count: 1458\n",
      "Trigram: ('Higgs', 'boson', 'decays'), Count: 1267\n",
      "Trigram: ('dark', 'model', 'scalar'), Count: 1065\n",
      "Trigram: ('TeV', 'mass', 'production'), Count: 1034\n",
      "Trigram: ('Higgs', 'boson', 'cross'), Count: 1189\n",
      "Trigram: ('field', 'quantum', 'theory'), Count: 1065\n",
      "Trigram: ('TeV', 'limits', 'mass'), Count: 1149\n",
      "Trigram: ('Higgs', 'analysis', 'boson'), Count: 1019\n",
      "Trigram: ('Higgs', 'boson', 'signal'), Count: 1027\n",
      "Trigram: ('mass', 'masses', 'model'), Count: 1125\n",
      "Trigram: ('GeV', 'Higgs', 'mass'), Count: 1289\n",
      "Trigram: ('GeV', 'mass', 'top'), Count: 1276\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "support_threshold_first_pass = 300  # Lower support threshold for the first pass\n",
    "support_threshold_second_pass = 1000  # Higher support threshold for the second pass\n",
    "hash_table_size = 10**10  # Large hash table to reduce collisions\n",
    "\n",
    "# Hash function to reduce collisions\n",
    "def hash_trigram(trigram, hash_table_size):\n",
    "    hash_obj = hashlib.sha256(str(trigram).encode())\n",
    "    return int(hash_obj.hexdigest(), 16) % hash_table_size\n",
    "\n",
    "# Normalize the trigram (sort to eliminate rotations)\n",
    "def normalize_trigram(trigram):\n",
    "    return tuple(sorted(trigram))\n",
    "\n",
    "# First pass: Count frequent words and populate hash table\n",
    "def first_pass(rdd, support_threshold_first_pass):\n",
    "    # Count occurrences of words\n",
    "    word_counts = rdd.flatMap(lambda record: record['abstract'].split()) \\\n",
    "                     .map(lambda word: (word, 1)) \\\n",
    "                     .reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "    # Find frequent words\n",
    "    frequent_words = word_counts.filter(lambda x: x[1] >= support_threshold_first_pass) \\\n",
    "                                .map(lambda x: x[0]) \\\n",
    "                                .collect()\n",
    "    frequent_word_set = set(frequent_words)\n",
    "\n",
    "    # Generate trigrams, hash them, and populate the hash table\n",
    "    hash_table = rdd.flatMap(\n",
    "        lambda record: [\n",
    "            hash_trigram(normalize_trigram(trigram), hash_table_size)\n",
    "            for trigram in combinations(record['abstract'].split(), 3)\n",
    "            if len(set(trigram)) == 3 and set(trigram).issubset(frequent_word_set)\n",
    "        ]\n",
    "    ).map(lambda bucket: (bucket, 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y) \\\n",
    "     .collectAsMap()\n",
    "\n",
    "    # Create bitmap based on hash table counts\n",
    "    bitmap = {bucket: (count >= support_threshold_first_pass) for bucket, count in hash_table.items()}\n",
    "    return frequent_word_set, bitmap\n",
    "\n",
    "# Second pass: Count candidate trigrams using the bitmap\n",
    "def second_pass(rdd, frequent_word_set, bitmap, support_threshold_second_pass):\n",
    "    def is_frequent_bucket(trigram):\n",
    "        bucket = hash_trigram(normalize_trigram(trigram), hash_table_size)\n",
    "        return bitmap.get(bucket, False)\n",
    "\n",
    "    # Generate and filter candidate trigrams\n",
    "    candidate_trigrams = rdd.flatMap(\n",
    "        lambda record: [\n",
    "            normalize_trigram(trigram)\n",
    "            for trigram in combinations(record['abstract'].split(), 3)\n",
    "            if len(set(trigram)) == 3 and set(trigram).issubset(frequent_word_set) and is_frequent_bucket(trigram)\n",
    "        ]\n",
    "    ).map(lambda trigram: (trigram, 1)) \\\n",
    "     .reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "    # Apply the second pass threshold\n",
    "    frequent_trigrams_temp = candidate_trigrams.filter(lambda x: x[1] >= support_threshold_second_pass)\n",
    "    return frequent_trigrams_temp\n",
    "\n",
    "\n",
    "# First pass: Lower threshold to capture more candidate trigrams\n",
    "frequent_word_set, bitmap = first_pass(sampled_rdd, support_threshold_first_pass)\n",
    "\n",
    "# Second pass: Apply a higher threshold to finalize frequent trigrams\n",
    "frequent_trigrams_PCY = second_pass(sampled_rdd, frequent_word_set, bitmap, support_threshold_second_pass)\n",
    "\n",
    "# Collect and display results\n",
    "results = frequent_trigrams_PCY.collect()\n",
    "for trigram, count in results:\n",
    "    print(f\"Trigram: {trigram}, Count: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Steps in the Algorithm**\n",
    "\n",
    "#### **1. Parameters**\n",
    "- **`support_threshold_first_pass`**: A lower threshold used in the first pass to identify potential candidates.\n",
    "- **`support_threshold_second_pass`**: A higher threshold used in the second pass to filter the final frequent trigrams.\n",
    "- **`hash_table_size`**: Size of the hash table to reduce collisions when hashing trigrams.\n",
    "\n",
    "#### **2. Utility Functions**\n",
    "- **`hash_trigram`**: Hashes a trigram using SHA-256 and maps it to a bucket in the hash table.\n",
    "- **`normalize_trigram`**: Sorts the words in a trigram to ensure consistency, treating `(A, B, C)` and `(C, B, A)` as the same.\n",
    "\n",
    "\n",
    "### **First Pass: Generating Frequent Words and Populating the Hash Table**\n",
    "1. **Word Counting**:\n",
    "   - Tokenizes abstracts into words and counts their occurrences using PySpark transformations.\n",
    "   - Filters words with counts below `support_threshold_first_pass` and stores the frequent words in a set.\n",
    "\n",
    "2. **Trigram Generation and Hashing**:\n",
    "   - Generates all trigrams from abstracts.\n",
    "   - Filters trigrams to include only those composed of frequent words from the first pass.\n",
    "   - Hashes each trigram into a bucket using `hash_trigram` and populates the hash table with bucket counts.\n",
    "\n",
    "3. **Bitmap Creation**:\n",
    "   - Converts the hash table into a bitmap where each bucket is marked as frequent (`True`) if its count meets the `support_threshold_first_pass`. This bitmap reduces memory usage and helps prune infrequent trigrams early.\n",
    "\n",
    "\n",
    "### **Second Pass: Filtering and Counting Candidate Trigrams**\n",
    "1. **Candidate Filtering Using Bitmap**:\n",
    "   - Generates trigrams from abstracts, ensuring:\n",
    "     - All words are frequent (from the first pass).\n",
    "     - The trigram maps to a bucket marked as frequent in the bitmap.\n",
    "\n",
    "2. **Trigram Counting**:\n",
    "   - Counts occurrences of the filtered trigrams across all abstracts.\n",
    "\n",
    "3. **Final Filtering**:\n",
    "   - Retains trigrams that meet the `support_threshold_second_pass`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Convert the results of both algorithms into sets of trigrams\n",
    "def extract_trigram_set(rdd):\n",
    "    return set(rdd.map(lambda x: x[0]).collect())  # Collect only the trigrams\n",
    "\n",
    "# Extract sets from both algorithms\n",
    "apriori_trigram_set = extract_trigram_set(frequent_trigrams_apriori)  # Replace with your A-priori RDD\n",
    "pcy_trigram_set = extract_trigram_set(frequent_trigrams_PCY)  # Replace with your PCY results\n",
    "\n",
    "# Step 2: Compute the Jaccard similarity\n",
    "intersection = apriori_trigram_set.intersection(pcy_trigram_set)\n",
    "union = apriori_trigram_set.union(pcy_trigram_set)\n",
    "jaccard_similarity = len(intersection) / len(union) if len(union) > 0 else 0\n",
    "\n",
    "# Step 3: Display the result\n",
    "print(f\"Jaccard Similarity: {jaccard_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"id\":\"0704.0001\",\"submitter\":\"Pavel Nadolsky\",\"authors\":\"C. Bal\\\\\\\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\"title\":\"Calculation of prompt diphoton production cross sections at Tevatron and\\\\n  LHC energies\",\"comments\":\"37 pages, 15 figures; published version\",\"journal-ref\":\"Phys.Rev.D76:013009,2007\",\"doi\":\"10.1103/PhysRevD.76.013009\",\"report-no\":\"ANL-HEP-PR-07-12\",\"categories\":\"hep-ph\",\"license\":null,\"abstract\":\"  A fully differential calculation in perturbative quantum chromodynamics is\\\\npresented for the production of massive photon pairs at hadron colliders. All\\\\nnext-to-leading order perturbative contributions from quark-antiquark,\\\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\\\nall-orders resummation of initial-state gluon radiation valid at\\\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\\\nspecified in which the calculation is most reliable. Good agreement is\\\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\\\nmore detailed tests with CDF and DO data. Predictions are shown for\\\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\\\nthat enhanced sensitivity to the signal can be obtained with judicious\\\\nselection of events.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 19:18:42 GMT\"},{\"version\":\"v2\",\"created\":\"Tue, 24 Jul 2007 20:10:27 GMT\"}],\"update_date\":\"2008-11-26\",\"authors_parsed\":[[\"Bal\\\\u00e1zs\",\"C.\",\"\"],[\"Berger\",\"E. L.\",\"\"],[\"Nadolsky\",\"P. M.\",\"\"],[\"Yuan\",\"C. -P.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0002\",\"submitter\":\"Louis Theran\",\"authors\":\"Ileana Streinu and Louis Theran\",\"title\":\"Sparsity-certifying Graph Decompositions\",\"comments\":\"To appear in Graphs and Combinatorics\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO cs.CG\",\"license\":\"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\",\"abstract\":\"  We describe a new algorithm, the $(k,\\\\\\\\ell)$-pebble game with colors, and use\\\\nit obtain a characterization of the family of $(k,\\\\\\\\ell)$-sparse graphs and\\\\nalgorithmic solutions to a family of problems concerning tree decompositions of\\\\ngraphs. Special instances of sparse graphs appear in rigidity theory and have\\\\nreceived increased attention in recent years. In particular, our colored\\\\npebbles generalize and strengthen the previous results of Lee and Streinu and\\\\ngive a new proof of the Tutte-Nash-Williams characterization of arboricity. We\\\\nalso present a new decomposition that certifies sparsity based on the\\\\n$(k,\\\\\\\\ell)$-pebble game with colors. Our work also exposes connections between\\\\npebble game algorithms and previous sparse graph algorithms by Gabow, Gabow and\\\\nWestermann and Hendrickson.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 02:26:18 GMT\"},{\"version\":\"v2\",\"created\":\"Sat, 13 Dec 2008 17:26:00 GMT\"}],\"update_date\":\"2008-12-13\",\"authors_parsed\":[[\"Streinu\",\"Ileana\",\"\"],[\"Theran\",\"Louis\",\"\"]]}',\n",
       " '{\"id\":\"0704.0003\",\"submitter\":\"Hongjun Pan\",\"authors\":\"Hongjun Pan\",\"title\":\"The evolution of the Earth-Moon system based on the dark matter field\\\\n  fluid model\",\"comments\":\"23 pages, 3 figures\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"physics.gen-ph\",\"license\":null,\"abstract\":\"  The evolution of Earth-Moon system is described by the dark matter field\\\\nfluid model proposed in the Meeting of Division of Particle and Field 2004,\\\\nAmerican Physical Society. The current behavior of the Earth-Moon system agrees\\\\nwith this model very well and the general pattern of the evolution of the\\\\nMoon-Earth system described by this model agrees with geological and fossil\\\\nevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5\\\\nbillion years ago, which is far beyond the Roche\\'s limit. The result suggests\\\\nthat the tidal friction may not be the primary cause for the evolution of the\\\\nEarth-Moon system. The average dark matter field fluid constant derived from\\\\nEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predicts\\\\nthat the Mars\\'s rotation is also slowing with the angular acceleration rate\\\\nabout -4.38 x 10^(-22) rad s^(-2).\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sun, 1 Apr 2007 20:46:54 GMT\"},{\"version\":\"v2\",\"created\":\"Sat, 8 Dec 2007 23:47:24 GMT\"},{\"version\":\"v3\",\"created\":\"Sun, 13 Jan 2008 00:36:28 GMT\"}],\"update_date\":\"2008-01-13\",\"authors_parsed\":[[\"Pan\",\"Hongjun\",\"\"]]}',\n",
       " '{\"id\":\"0704.0004\",\"submitter\":\"David Callan\",\"authors\":\"David Callan\",\"title\":\"A determinant of Stirling cycle numbers counts unlabeled acyclic\\\\n  single-source automata\",\"comments\":\"11 pages\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO\",\"license\":null,\"abstract\":\"  We show that a determinant of Stirling cycle numbers counts unlabeled acyclic\\\\nsingle-source automata. The proof involves a bijection from these automata to\\\\ncertain marked lattice paths and a sign-reversing involution to evaluate the\\\\ndeterminant.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 03:16:14 GMT\"}],\"update_date\":\"2007-05-23\",\"authors_parsed\":[[\"Callan\",\"David\",\"\"]]}',\n",
       " '{\"id\":\"0704.0005\",\"submitter\":\"Alberto Torchinsky\",\"authors\":\"Wael Abu-Shammala and Alberto Torchinsky\",\"title\":\"From dyadic $\\\\\\\\Lambda_{\\\\\\\\alpha}$ to $\\\\\\\\Lambda_{\\\\\\\\alpha}$\",\"comments\":null,\"journal-ref\":\"Illinois J. Math. 52 (2008) no.2, 681-689\",\"doi\":null,\"report-no\":null,\"categories\":\"math.CA math.FA\",\"license\":null,\"abstract\":\"  In this paper we show how to compute the $\\\\\\\\Lambda_{\\\\\\\\alpha}$ norm, $\\\\\\\\alpha\\\\\\\\ge\\\\n0$, using the dyadic grid. This result is a consequence of the description of\\\\nthe Hardy spaces $H^p(R^N)$ in terms of dyadic and special atoms.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 18:09:58 GMT\"}],\"update_date\":\"2013-10-15\",\"authors_parsed\":[[\"Abu-Shammala\",\"Wael\",\"\"],[\"Torchinsky\",\"Alberto\",\"\"]]}',\n",
       " '{\"id\":\"0704.0006\",\"submitter\":\"Yue Hin Pong\",\"authors\":\"Y. H. Pong and C. K. Law\",\"title\":\"Bosonic characters of atomic Cooper pairs across resonance\",\"comments\":\"6 pages, 4 figures, accepted by PRA\",\"journal-ref\":null,\"doi\":\"10.1103/PhysRevA.75.043613\",\"report-no\":null,\"categories\":\"cond-mat.mes-hall\",\"license\":null,\"abstract\":\"  We study the two-particle wave function of paired atoms in a Fermi gas with\\\\ntunable interaction strengths controlled by Feshbach resonance. The Cooper pair\\\\nwave function is examined for its bosonic characters, which is quantified by\\\\nthe correction of Bose enhancement factor associated with the creation and\\\\nannihilation composite particle operators. An example is given for a\\\\nthree-dimensional uniform gas. Two definitions of Cooper pair wave function are\\\\nexamined. One of which is chosen to reflect the off-diagonal long range order\\\\n(ODLRO). Another one corresponds to a pair projection of a BCS state. On the\\\\nside with negative scattering length, we found that paired atoms described by\\\\nODLRO are more bosonic than the pair projected definition. It is also found\\\\nthat at $(k_F a)^{-1} \\\\\\\\ge 1$, both definitions give similar results, where more\\\\nthan 90% of the atoms occupy the corresponding molecular condensates.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 04:24:59 GMT\"}],\"update_date\":\"2015-05-13\",\"authors_parsed\":[[\"Pong\",\"Y. H.\",\"\"],[\"Law\",\"C. K.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0007\",\"submitter\":\"Alejandro Corichi\",\"authors\":\"Alejandro Corichi, Tatjana Vukasinac and Jose A. Zapata\",\"title\":\"Polymer Quantum Mechanics and its Continuum Limit\",\"comments\":\"16 pages, no figures. Typos corrected to match published version\",\"journal-ref\":\"Phys.Rev.D76:044016,2007\",\"doi\":\"10.1103/PhysRevD.76.044016\",\"report-no\":\"IGPG-07/03-2\",\"categories\":\"gr-qc\",\"license\":null,\"abstract\":\"  A rather non-standard quantum representation of the canonical commutation\\\\nrelations of quantum mechanics systems, known as the polymer representation has\\\\ngained some attention in recent years, due to its possible relation with Planck\\\\nscale physics. In particular, this approach has been followed in a symmetric\\\\nsector of loop quantum gravity known as loop quantum cosmology. Here we explore\\\\ndifferent aspects of the relation between the ordinary Schroedinger theory and\\\\nthe polymer description. The paper has two parts. In the first one, we derive\\\\nthe polymer quantum mechanics starting from the ordinary Schroedinger theory\\\\nand show that the polymer description arises as an appropriate limit. In the\\\\nsecond part we consider the continuum limit of this theory, namely, the reverse\\\\nprocess in which one starts from the discrete theory and tries to recover back\\\\nthe ordinary Schroedinger quantum mechanics. We consider several examples of\\\\ninterest, including the harmonic oscillator, the free particle and a simple\\\\ncosmological model.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 04:27:22 GMT\"},{\"version\":\"v2\",\"created\":\"Wed, 22 Aug 2007 22:42:11 GMT\"}],\"update_date\":\"2008-11-26\",\"authors_parsed\":[[\"Corichi\",\"Alejandro\",\"\"],[\"Vukasinac\",\"Tatjana\",\"\"],[\"Zapata\",\"Jose A.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0008\",\"submitter\":\"Damian Swift\",\"authors\":\"Damian C. Swift\",\"title\":\"Numerical solution of shock and ramp compression for general material\\\\n  properties\",\"comments\":\"Minor corrections\",\"journal-ref\":\"Journal of Applied Physics, vol 104, 073536 (2008)\",\"doi\":\"10.1063/1.2975338\",\"report-no\":\"LA-UR-07-2051, LLNL-JRNL-410358\",\"categories\":\"cond-mat.mtrl-sci\",\"license\":\"http://arxiv.org/licenses/nonexclusive-distrib/1.0/\",\"abstract\":\"  A general formulation was developed to represent material models for\\\\napplications in dynamic loading. Numerical methods were devised to calculate\\\\nresponse to shock and ramp compression, and ramp decompression, generalizing\\\\nprevious solutions for scalar equations of state. The numerical methods were\\\\nfound to be flexible and robust, and matched analytic results to a high\\\\naccuracy. The basic ramp and shock solution methods were coupled to solve for\\\\ncomposite deformation paths, such as shock-induced impacts, and shock\\\\ninteractions with a planar interface between different materials. These\\\\ncalculations capture much of the physics of typical material dynamics\\\\nexperiments, without requiring spatially-resolving simulations. Example\\\\ncalculations were made of loading histories in metals, illustrating the effects\\\\nof plastic work on the temperatures induced in quasi-isentropic and\\\\nshock-release experiments, and the effect of a phase transition.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 04:47:20 GMT\"},{\"version\":\"v2\",\"created\":\"Thu, 10 Apr 2008 08:42:28 GMT\"},{\"version\":\"v3\",\"created\":\"Tue, 1 Jul 2008 18:54:28 GMT\"}],\"update_date\":\"2009-02-05\",\"authors_parsed\":[[\"Swift\",\"Damian C.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0009\",\"submitter\":\"Paul Harvey\",\"authors\":\"Paul Harvey, Bruno Merin, Tracy L. Huard, Luisa M. Rebull, Nicholas\\\\n  Chapman, Neal J. Evans II, Philip C. Myers\",\"title\":\"The Spitzer c2d Survey of Large, Nearby, Insterstellar Clouds. IX. The\\\\n  Serpens YSO Population As Observed With IRAC and MIPS\",\"comments\":null,\"journal-ref\":\"Astrophys.J.663:1149-1173,2007\",\"doi\":\"10.1086/518646\",\"report-no\":null,\"categories\":\"astro-ph\",\"license\":null,\"abstract\":\"  We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacy\\\\nobservations of the Serpens star-forming region. In particular we present a set\\\\nof criteria for isolating bona fide young stellar objects, YSO\\'s, from the\\\\nextensive background contamination by extra-galactic objects. We then discuss\\\\nthe properties of the resulting high confidence set of YSO\\'s. We find 235 such\\\\nobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. An\\\\nadditional set of 51 lower confidence YSO\\'s outside this area is identified\\\\nfrom the MIPS data combined with 2MASS photometry. We describe two sets of\\\\nresults, color-color diagrams to compare our observed source properties with\\\\nthose of theoretical models for star/disk/envelope systems and our own modeling\\\\nof the subset of our objects that appear to be star+disks. These objects\\\\nexhibit a very wide range of disk properties, from many that can be fit with\\\\nactively accreting disks to some with both passive disks and even possibly\\\\ndebris disks. We find that the luminosity function of YSO\\'s in Serpens extends\\\\ndown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.\\\\nThe lower limit may be set by our inability to distinguish YSO\\'s from\\\\nextra-galactic sources more than by the lack of YSO\\'s at very low luminosities.\\\\nA spatial clustering analysis shows that the nominally less-evolved YSO\\'s are\\\\nmore highly clustered than the later stages and that the background\\\\nextra-galactic population can be fit by the same two-point correlation function\\\\nas seen in other extra-galactic studies. We also present a table of matches\\\\nbetween several previous infrared and X-ray studies of the Serpens YSO\\\\npopulation and our Spitzer data set.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Mon, 2 Apr 2007 19:41:34 GMT\"}],\"update_date\":\"2010-03-18\",\"authors_parsed\":[[\"Harvey\",\"Paul\",\"\"],[\"Merin\",\"Bruno\",\"\"],[\"Huard\",\"Tracy L.\",\"\"],[\"Rebull\",\"Luisa M.\",\"\"],[\"Chapman\",\"Nicholas\",\"\"],[\"Evans\",\"Neal J.\",\"II\"],[\"Myers\",\"Philip C.\",\"\"]]}',\n",
       " '{\"id\":\"0704.0010\",\"submitter\":\"Sergei Ovchinnikov\",\"authors\":\"Sergei Ovchinnikov\",\"title\":\"Partial cubes: structures, characterizations, and constructions\",\"comments\":\"36 pages, 17 figures\",\"journal-ref\":null,\"doi\":null,\"report-no\":null,\"categories\":\"math.CO\",\"license\":null,\"abstract\":\"  Partial cubes are isometric subgraphs of hypercubes. Structures on a graph\\\\ndefined by means of semicubes, and Djokovi\\\\\\\\\\'{c}\\'s and Winkler\\'s relations play\\\\nan important role in the theory of partial cubes. These structures are employed\\\\nin the paper to characterize bipartite graphs and partial cubes of arbitrary\\\\ndimension. New characterizations are established and new proofs of some known\\\\nresults are given.\\\\n  The operations of Cartesian product and pasting, and expansion and\\\\ncontraction processes are utilized in the paper to construct new partial cubes\\\\nfrom old ones. In particular, the isometric and lattice dimensions of finite\\\\npartial cubes obtained by means of these operations are calculated.\\\\n\",\"versions\":[{\"version\":\"v1\",\"created\":\"Sat, 31 Mar 2007 05:10:16 GMT\"}],\"update_date\":\"2007-05-23\",\"authors_parsed\":[[\"Ovchinnikov\",\"Sergei\",\"\"]]}']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_rdd = sc.textFile(\"MDA2024-Arxiv-Dataset.json\")\n",
    "arxiv_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON string\n",
    "def parse_json(line):\n",
    "    return json.loads(line)\n",
    "\n",
    "parsed_rdd = arxiv_rdd.map(parse_json).filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify and remove or impute null values\n",
    "def remove_nulls(record):\n",
    "    # Check for null or empty fields in the record\n",
    "    if all(record.values()):  # Ensure no field is None or empty\n",
    "        return record\n",
    "    else:\n",
    "        return None  # Ignore records with null or empty fields\n",
    "\n",
    "cleaned_rdd = parsed_rdd.filter(lambda x: remove_nulls(x) is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of stopwords for removal\n",
    "stopwords = set(StopWordsRemover.loadDefaultStopWords(\"english\"))\n",
    "\n",
    "def remove_stopwords(record):\n",
    "    for key, value in record.items():\n",
    "        if isinstance(value, str):  # Process only text fields\n",
    "            words = value.split()\n",
    "            filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "            record[key] = \" \".join(filtered_words)\n",
    "    return record\n",
    "\n",
    "stopwords_removed_rdd = cleaned_rdd.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '07040008',\n",
       "  'submitter': 'Damian Swift',\n",
       "  'authors': 'Damian C Swift',\n",
       "  'title': 'Numerical solution shock ramp compression general material properties',\n",
       "  'comments': 'Minor corrections',\n",
       "  'journal-ref': 'Journal Applied Physics vol 104 073536 2008',\n",
       "  'doi': '10106312975338',\n",
       "  'report-no': 'LAUR072051 LLNLJRNL410358',\n",
       "  'categories': 'condmatmtrlsci',\n",
       "  'license': 'httparxivorglicensesnonexclusivedistrib10',\n",
       "  'abstract': 'general formulation developed represent material models applications dynamic loading Numerical methods devised calculate response shock ramp compression ramp decompression generalizing previous solutions scalar equations state numerical methods found flexible robust matched analytic results high accuracy basic ramp shock solution methods coupled solve composite deformation paths shockinduced impacts shock interactions planar interface different materials calculations capture much physics typical material dynamics experiments without requiring spatiallyresolving simulations Example calculations made loading histories metals illustrating effects plastic work temperatures induced quasiisentropic shockrelease experiments effect phase transition',\n",
       "  'versions': [{'version': 'v1', 'created': 'Sat, 31 Mar 2007 04:47:20 GMT'},\n",
       "   {'version': 'v2', 'created': 'Thu, 10 Apr 2008 08:42:28 GMT'},\n",
       "   {'version': 'v3', 'created': 'Tue, 1 Jul 2008 18:54:28 GMT'}],\n",
       "  'update_date': '20090205',\n",
       "  'authors_parsed': [['Swift', 'Damian C.', '']]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_useless_characters(record):\n",
    "    for key, value in record.items():\n",
    "        if isinstance(value, str):  # Process only text fields\n",
    "            # Remove special characters, retaining only alphanumeric and spaces\n",
    "            record[key] = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", value)\n",
    "    return record\n",
    "\n",
    "final_cleaned_rdd = stopwords_removed_rdd.map(remove_useless_characters)\n",
    "\n",
    "# Save or view the final cleaned RDD\n",
    "final_cleaned_rdd.take(1)  # View the first cleaned records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records in sampled RDD (50.0%): 20394\n"
     ]
    }
   ],
   "source": [
    "# Get a 1% sample from final_cleaned_rdd\n",
    "sample_fraction = 0.5\n",
    "sampled_rdd = final_cleaned_rdd.sample(withReplacement=False, fraction=sample_fraction, seed=42)\n",
    "\n",
    "# Check how many records are in the sampled RDD\n",
    "sampled_count = sampled_rdd.count()\n",
    "print(f\"Number of records in sampled RDD ({sample_fraction*100}%): {sampled_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query Article ID: 08033126\n",
      "Similar Articles:\n",
      "Article ID: 08033127, Similarity: 1.00\n",
      "\n",
      "Query Article ID: 08033127\n",
      "Similar Articles:\n",
      "Article ID: 08033126, Similarity: 1.00\n",
      "\n",
      "Query Article ID: 08040080\n",
      "Similar Articles:\n",
      "Article ID: 08040090, Similarity: 1.00\n",
      "Article ID: 08040093, Similarity: 1.00\n",
      "\n",
      "Query Article ID: 08040090\n",
      "Similar Articles:\n",
      "Article ID: 08040093, Similarity: 1.00\n",
      "Article ID: 08040080, Similarity: 1.00\n",
      "\n",
      "Query Article ID: 08040093\n",
      "Similar Articles:\n",
      "Article ID: 08040090, Similarity: 1.00\n",
      "Article ID: 08040080, Similarity: 1.00\n",
      "\n",
      "Query Article ID: 08042962\n",
      "Similar Articles:\n",
      "Article ID: 08042970, Similarity: 1.00\n",
      "Article ID: 08042973, Similarity: 0.80\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_hashes = 1000  # Increased number of hash functions for MinHash\n",
    "r = 10            # Rows per band\n",
    "b = 100           # Number of bands (num_hashes = r * b)\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "def tokenize(text):\n",
    "    return text.lower().split() if text else []\n",
    "\n",
    "def generate_shingles(tokens, k=5):\n",
    "    if len(tokens) < k:\n",
    "        return set()\n",
    "    return set([''.join(tokens[i:i + k]) for i in range(len(tokens) - k + 1)])\n",
    "\n",
    "# Step 2: Generate MinHash Signatures\n",
    "def minhash_signature(shingles, hash_functions):\n",
    "    if not shingles:\n",
    "        return [float('inf')] * len(hash_functions)\n",
    "    signature = []\n",
    "    for a, b, p in hash_functions:\n",
    "        min_hash = min(((a * hash(s) + b) % p) for s in shingles)\n",
    "        signature.append(min_hash)\n",
    "    return signature\n",
    "\n",
    "# Generate hash functions\n",
    "def generate_hash_functions(num_hashes):\n",
    "    p = 2**31 - 1  # A large prime number\n",
    "    hash_functions = [(random.randint(1, p), random.randint(0, p), p) for _ in range(num_hashes)]\n",
    "    return hash_functions\n",
    "\n",
    "# Step 3: Locality Sensitive Hashing\n",
    "def lsh_buckets(signature, r, b):\n",
    "    bands = []\n",
    "    for i in range(b):\n",
    "        start = i * r\n",
    "        end = start + r\n",
    "        band = tuple(signature[start:end])\n",
    "        bands.append(hash(band))\n",
    "    return bands\n",
    "\n",
    "# Step 4: Retrieve Similar Articles\n",
    "def jaccard_similarity(set1, set2):\n",
    "    return len(set1 & set2) / len(set1 | set2) if set1 and set2 else 0\n",
    "\n",
    "def find_similar_articles(lsh_index, article_id, shingles, dataset, threshold=0.5):\n",
    "    if article_id not in lsh_index:\n",
    "        return []\n",
    "    \n",
    "    candidates = set()\n",
    "    for band_hash in lsh_index[article_id]:\n",
    "        if band_hash in reverse_index:\n",
    "            candidates.update(reverse_index[band_hash])\n",
    "    candidates.discard(article_id)\n",
    "    \n",
    "    similar_articles = []\n",
    "    query_shingles = shingles.get(article_id, set())\n",
    "    for candidate in candidates:\n",
    "        sim = jaccard_similarity(query_shingles, shingles.get(candidate, set()))\n",
    "        if sim >= threshold:\n",
    "            similar_articles.append((candidate, sim))\n",
    "    return sorted(similar_articles, key=lambda x: -x[1])\n",
    "\n",
    "# Main Workflow\n",
    "articles = sampled_rdd.map(lambda x: (x['id'], generate_shingles(tokenize(x.get('abstract', ''))))).filter(lambda x: x[1]).collect()\n",
    "shingles = {article[0]: article[1] for article in articles}\n",
    "\n",
    "# Generate MinHash signatures\n",
    "hash_functions = generate_hash_functions(num_hashes)\n",
    "signatures = {article_id: minhash_signature(shingle_set, hash_functions) for article_id, shingle_set in shingles.items()}\n",
    "\n",
    "# Create LSH index\n",
    "lsh_index = {}\n",
    "reverse_index = {}\n",
    "for article_id, signature in signatures.items():\n",
    "    bands = lsh_buckets(signature, r, b)\n",
    "    lsh_index[article_id] = bands\n",
    "    for band in bands:\n",
    "        if band not in reverse_index:\n",
    "            reverse_index[band] = set()\n",
    "        reverse_index[band].add(article_id)\n",
    "\n",
    "# Find and print similarities for the first 6 papers\n",
    "counter = 0\n",
    "for article_id in shingles.keys():\n",
    "    if counter >= 6:\n",
    "        break\n",
    "    similar_articles = find_similar_articles(lsh_index, article_id, shingles, articles)\n",
    "    if similar_articles:\n",
    "        print(f\"\\nQuery Article ID: {article_id}\")\n",
    "        print(\"Similar Articles:\")\n",
    "        for similar_id, similarity in similar_articles:\n",
    "            print(f\"Article ID: {similar_id}, Similarity: {similarity:.2f}\")\n",
    "        counter += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preprocessing**\n",
    "- **Tokenization**:\n",
    "  - Splits the text of each paper (e.g., abstract) into tokens (words).\n",
    "  - Converts all text to lowercase for case-insensitive comparisons.\n",
    "  \n",
    "- **Shingles Generation**:\n",
    "  - Creates overlapping substrings (shingles) of length (k) from the tokens.\n",
    "  - Example: For tokens [\"`this`\", \"`is`\", \"`a`\", \"`test`\"] and (k=3), shingles are: {\"`thisisatest`\"}\n",
    "\n",
    "\n",
    "### **2. MinHashing**\n",
    "  - **Generate Random Hash Functions**:\n",
    "     - h(x) = (a.x + b) % p\n",
    "  - **Create MinHash Signatures**:\n",
    "     - For each paper's shingles, compute the minimum hash value under each of the hash functions.\n",
    "     - A signature is an array of these minimum hash values.\n",
    "\n",
    "MinHashing ensures that the similarity between two sets of shingles is preserved.\n",
    "\n",
    "### **3. Locality-Sensitive Hashing (LSH)**\n",
    "- **Purpose**:\n",
    "  - Efficiently group and retrieve similar items by hashing similar MinHash signatures into the same buckets.\n",
    "\n",
    "- **Steps**:\n",
    "  1. **Split MinHash Signatures into Bands**:\n",
    "     - Divide the signature into (b) bands, each containing (r) rows.\n",
    "  2. **Hash Each Band**:\n",
    "     - For each band (subarray of the signature), compute a hash.\n",
    "     - Papers with the same hash for a band are likely to be similar.\n",
    "\n",
    "\n",
    "### **4. Query for Similar Papers**\n",
    "- **Jaccard Similarity**:\n",
    "  - Measures the similarity between two sets of shingles: J(A, B) = Intersection(A,B) / Union(A,B)\n",
    "  - If the similarity exceeds a threshold (e.g., 0.5 ), the papers are considered similar.\n",
    "\n",
    "- **Steps**:\n",
    "  1. Identify \"candidate\" papers using LSH:\n",
    "     - For the query paper, look up its bands in the LSH index to find potential matches.\n",
    "  2. Compute Jaccard Similarity for these candidates.\n",
    "\n",
    "\n",
    "### **5. Main Workflow**\n",
    "1. **Data Preparation**:\n",
    "   - Tokenize and shingle all papers in the dataset.\n",
    "   - Compute MinHash signatures and build the LSH index.\n",
    "2. **Query Similarity**:\n",
    "   - For each paper, retrieve candidate matches using LSH.\n",
    "   - Calculate and rank similar papers based on Jaccard Similarity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
